---
layout: post
title:  "DCTTS를 이용한 손석희 앵커 목소리 흉내내기"
date:   2018-05-21 19:00:00 +0700
author: Jin
categories: SpeechSynthesis
tags:	TTS SpeechSynthesis GenerativeModel DCTTS 손석희
cover:  "/assets/instacode.png"
---

+	DCTTS 오픈소스와 JTBC 뉴스룸 손석희 앵커 데이터를 이용한 학습 결과

## DCTTS
+	Neural Speech Synthesis Model은 2016년에 구글의 WaveNet이 나온 이후 17년도에 상당히 좋은 모델이 우후죽순 쏟아져 나왔다.(WaveNet은 사실상 Vocoder로 쓰고있다)
+	대부분의 end-to-end neural speech synthesis model이 RNN을 사용했으나, DCTTS와 DeepVoice3는 RNN을 사용하지 않고 Convolution으로 구성했다. 
+	그 중에서도 상당히 빠른 시간에 학습하면서(Tacotron보다 수 배 이상 빠르다), 학습 완료후 Synthesize를 할 때도 합리적인 시간 내에 wav를 만들어 낸다. 물론, 성능도 Training Data만 좋으면 크게 거슬리지 않는다.(전문가가 아닌 일반이이 듣기에는)
+	DCTTS model 코드는 카카오브레인의 Kyubyong님의 코드가 가장 깔끔하게 pytorch로 쓰여져있다.


## 손석희 앵커의 JTBC 뉴스룸 데이터
+	이전까지는 대부분이 LJSpeech 혹은 Blizzard Challenge dataset 등을 사용한 영어 기반의 TTS 모델들과 Sample들만 돌아다녔다.
+	그러나 Taehoon Kim(github carpedm20)님이 Multi-speaker tacotron tensorflow를 Github에 업로드함과 동시에 샘플을 본인 홈페이지에 올리면서(손석희 앵커 외 유명인(?) 2명) 엄청난 주목을 받았다. 다소 noise와 기계음은 있었지만, 단순히 웹 상의 데이터를 크롤링하고 손쉽게(?) Tensorflow로 학습하여 그럴듯한 목소리의 오디오를 만들어 냈다. 물론, 전체 코드는 그렇게 단순하지 않다. 모델 코드는 비교적 쉬운편.
+	현재는 저작권 문제로 일부 코드와 데이터가 제거되었으나, 손석희 앵커의 데이터를 크롤링하는 코드는 아직 있고 아주 유용하다. 이런 코드는 직접 짜려면 정말 많은 시행착오를 필요로 한다...


## 샘플 오디오
+	Kyubyong님의 DCTTS 모델과 Taehoon님의 크롤링 데이터를 이용해 학습한 모델이다.
+	전처리 작업을 몇가지 테스트 중이나 기본적인 몇가지만 넣은 상태이므로 개선의 여지가 있다.
+	다른 모델도 마찬가지겠지만, 데이터의 Quality가 상당히 중요해 보인다. Audio fidelity뿐만 아니라, transcript도...

<audio ref='themeSong' src="https://raw.githubusercontent.com/yangyangii/yangyangii.github.io/master/assets/_posts/audios/son_1.mp3
" autoPlay loop></audio>


## References
+   <em>[DCTTS [Tachibana, H., et al./ 2017]](https://arxiv.org/pdf/1710.08969)</em>
+	<em>[Kyubyong's KSS](https://github.com/Kyubyong/kss)</em>
+	<em>[carpedm20's Multi-speaker-tacotron-tensorflow](https://github.com/carpedm20/multi-speaker-tacotron-tensorflow)</em>